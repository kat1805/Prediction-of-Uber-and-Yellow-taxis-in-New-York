{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kasturi/opt/anaconda3/lib/python3.9/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 23:10:02 WARN Utils: Your hostname, MacBook-Air-3.local resolves to a loopback address: 127.0.0.1; using 192.168.0.66 instead (on interface en0)\n",
      "22/08/21 23:10:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 23:10:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/21 23:10:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/21 23:10:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/08/21 23:10:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZon\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged training set\n",
    "\n",
    "merged_train = spark.read.parquet('../data/curated/merged_training.paraquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geospatial data\n",
    "\n",
    "sf = gpd.read_file(\"../data/raw/taxi_zones.shp\")\n",
    "zones = spark.read.option(\"header\", True).csv(\"../data/raw/nyc.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the training data with the boroughs\n",
    "\n",
    "merged_train = merged_train \\\n",
    "    .join(zones, merged_train.PULocationID == zones.LocationID, \"inner\") \\\n",
    "    .drop('LocationID', 'Zone', \"service_zone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the external covid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Covid-19 dataset\n",
    "\n",
    "covid_data_county = spark.read.option(\"header\", True).csv(\"../data/raw/covid_county.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the counties with the boroughs\n",
    "\n",
    "covid_data_county = covid_data_county.withColumn('Borough', \n",
    "    when(((F.col('County') == 'Bronx')), 'Bronx') \\\n",
    "    .when(((F.col('County') == 'Kings')), 'Brooklyn') \\\n",
    "    .when(((F.col('County') == 'New York')), 'Manhattan') \\\n",
    "    .when(((F.col('County') == 'Queens')), 'Queens') \\\n",
    "    .when(((F.col('County') == 'Richmond')), 'Staten Island') \\\n",
    "    .when(((F.col('County') == 'Essex')), 'EWR') \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null and nan values\n",
    "\n",
    "cols = ['Test Date', 'New Positives', 'Test % Positive']\n",
    "\n",
    "covid_data_county.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    " for c in cols ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "\n",
    "covid_data_county = covid_data_county.withColumnRenamed(\n",
    "\n",
    "   'New Positives',\n",
    "    \"covid_cases\"\n",
    ")\n",
    "covid_data_county = covid_data_county.withColumnRenamed(\n",
    "\n",
    "   'Total Number of Tests Performed',\n",
    "    \"total_tests\"\n",
    ")\n",
    "\n",
    "covid_data_county = covid_data_county.withColumnRenamed(\n",
    "\n",
    "   'Test % Positive',\n",
    "    \"positivity_rate\"\n",
    ")\n",
    "\n",
    "# Converting the dates in a timestampformat\n",
    "\n",
    "covid_data_county = covid_data_county.select(\n",
    "    \n",
    "    \"Test Date\", \"covid_cases\", \"Borough\", \"total_tests\", \"positivity_rate\",\n",
    "    \n",
    "    from_unixtime(unix_timestamp( 'Test Date', 'MM/dd/yyy')).alias('Date')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the string timestamp at an actual timestamp\n",
    "\n",
    "covid_data_county = covid_data_county.withColumn(\"date\",\n",
    "\n",
    "    F.col(\"Date\").cast('TIMESTAMP')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year, month, day from the timestamp\n",
    "\n",
    "covid_data_county = covid_data_county.withColumn(\"Year\", \n",
    "date_format('date', 'yyyy'))\n",
    "\n",
    "covid_data_county  = covid_data_county.withColumn(\"Month\", \n",
    "date_format('date', 'MMMM'))\n",
    "\n",
    "covid_data_county  = covid_data_county.withColumn(\"Day\", \n",
    "date_format('date', 'dd'))\n",
    "\n",
    "# Dropping the timestamp column\n",
    "covid_data_county = covid_data_county.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the merged train and covid dataset by montj\n",
    "\n",
    "covid_data_county = covid_data_county.orderBy(col(\"Month\").asc(), \n",
    "col(\"Day\").asc())\n",
    "\n",
    "\n",
    "merged_train = merged_train.orderBy(col(\"Month\").asc(), col(\"Date\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a new column based on month, date and borough to merge the train and \n",
    "# covid dataset\n",
    "\n",
    "merged_train = merged_train.withColumn(\"day_and_date\"\n",
    ",\n",
    "F.concat(F.col(\"Month\"), F.col(\"Date\"), F.col(\"Borough\"))\n",
    ")\n",
    "\n",
    "covid_data_county = covid_data_county.withColumn(\"day_and_date\"\n",
    ",\n",
    "F.concat(F.col(\"Month\"), F.col(\"Day\"), F.col(\"Borough\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any missing values\n",
    "\n",
    "covid_data_county.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "for c in covid_data_county.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the positivity rate from the column\n",
    "\n",
    "covid_data_county = covid_data_county.withColumn(\"positivity_rate\", \n",
    "\n",
    "F.regexp_extract(covid_data_county.positivity_rate, r'\\d\\.\\d\\d', idx=0))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the positivity rate column to a double\n",
    "\n",
    "covid_data_county = covid_data_county.withColumn(\n",
    "    \"positivity_rate\",\n",
    "\n",
    "    F.col( \"positivity_rate\").cast('DOUBLE')\n",
    ")\n",
    "\n",
    "# Converting the other numeric columns to integer\n",
    "\n",
    "for field in ('covid_cases', 'total_tests'):\n",
    "\n",
    "    covid_data_county = covid_data_county.withColumn(\n",
    "        field,\n",
    "\n",
    "        F.col(field).cast('INT')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns in the covid dataset \n",
    "\n",
    "covid_data_county = covid_data_county.withColumnRenamed(\"Borough\", \n",
    "\"Borough_dup\")\n",
    "\n",
    "covid_data_county = covid_data_county.withColumnRenamed(\"Year\", \"Year_dup\")\n",
    "\n",
    "covid_data_county = covid_data_county.withColumnRenamed(\"Month\", \"Month_dup\")\n",
    "\n",
    "covid_data_county = covid_data_county.withColumnRenamed(\"Day\", \"Day_dup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the covid dataset based on year value of 2021 and 2022 for the\n",
    "# train and test sets respectively\n",
    "\n",
    "covid_data_county.createOrReplaceTempView(\"covid_temp\")\n",
    "\n",
    "covid_data_county_train = spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM \n",
    "    covid_temp\n",
    "WHERE \n",
    "    Year_dup = 2021 AND Borough_dup IS NOT NULL\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "covid_data_county_test = spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM \n",
    "    covid_temp\n",
    "WHERE \n",
    "    Year_dup = 2022 AND Borough_dup IS NOT NULL\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Covid-19 and training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the covid data with train data to prepare the final train dataset\n",
    "\n",
    "final_train_data = merged_train \\\n",
    "    .join(covid_data_county_train, \n",
    "    merged_train.day_and_date == covid_data_county_train.day_and_date, \n",
    "    \"inner\")\\\n",
    "    .drop(\"day_and_date\", \"Borough_dup\", \"Year_dup\", \"Month_dup\", \"Day_dup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the locations ids back to string\n",
    "\n",
    "for field in ('PU', 'DO'):\n",
    "    field = f'{field}LocationID'\n",
    "    final_train_data = final_train_data.withColumn(\n",
    "        field,\n",
    "        F.col(field).cast('STRING')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving the final training dataset\n",
    "\n",
    "final_train_data.write.parquet(\"../data/curated/final_train_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the curated covid dataset for train and test \n",
    "\n",
    "covid_data_county_train.write.parquet(\"../data/curated/covid_curated_train.parquet\")\n",
    "covid_data_county_test.write.parquet(\"../data/curated/covid_curated_test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afcda84b7471a9b9fde108a34f159f021985318d3feb99cad4970c959fa9ac9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
