{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZon\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow Taxi Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the yellow taxis raw train data directory\n",
    "main_dir = '../data/raw/raw_train/yellow/'\n",
    "mth = range(1,13)\n",
    "yr = '2021'\n",
    "\n",
    "# Define the schema for the spark dataframe\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "sch = spark.read.parquet('../data/raw/raw_train/yellow/2021-01.parquet')\n",
    "sdf_yellow_train = spark.createDataFrame(emptyRDD, sch.schema )\n",
    "\n",
    "\n",
    "# Merge the data from 2021 into a single spark dataframe\n",
    "for month in mth:\n",
    "    if month < 10:\n",
    "         month = str(month).zfill(2)\n",
    "    sdf = spark.read.parquet(f'{main_dir}{yr}-{month}.parquet')\n",
    "\n",
    "# The airport_fee column has different data types in different files\n",
    "# Hence converting into a same data type and joining the dataframes \n",
    "# into a single dataframe\n",
    "\n",
    "    sdf_updated = sdf.withColumn(\n",
    "        'airport_fee',\n",
    "        F.col('airport_fee').cast('DOUBLE')\n",
    "    )\n",
    "    sdf_yellow_train = sdf_yellow_train.unionByName(sdf_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:======================================================> (90 + 2) / 92]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|        1478695|            0|   1478695|           1478695|           0|           0|           0|          0|    0|      0|         0|           0|                    0|           0|             1478695|    5641418|\n",
      "+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking for the null and nan values\n",
    "\n",
    "sdf_yellow_train.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "for c in (sdf_yellow_train.columns)[3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the null values in the airport_fee and congestion surcharge column\n",
    "#  with 0s\n",
    "\n",
    "sdf_yellow_train = sdf_yellow_train.fillna(value = 0.0, subset=['airport_fee',\n",
    " 'congestion_surcharge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:======================================================> (90 + 2) / 92]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|              0|            0|         0|                 0|           0|           0|           0|          0|    0|      0|         0|           0|                    0|           0|                   0|          0|\n",
      "+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Dropping the rest of the null values from dataframe\n",
    "\n",
    "sdf_yellow_train = sdf_yellow_train.dropna()\n",
    "\n",
    "# Checking if the dataframe has any other missing values\n",
    "sdf_yellow_train.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    " for c in (sdf_yellow_train.columns)[3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the pickup and dropoff location columns to an integer\n",
    "\n",
    "for field in ('PU', 'DO'):\n",
    "    field = f'{field}LocationID'\n",
    "    sdf_yellow_train = sdf_yellow_train.withColumn(\n",
    "        field,\n",
    "        F.col(field).cast('INT')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for valid records based on the following conditions\n",
    "\n",
    "sdf_yellow_train = sdf_yellow_train.withColumn(\n",
    "    'is_valid_record',\n",
    "    \n",
    "    F.when(\n",
    "        ((F.col('total_amount') > 0) & (F.col('PULocationID').between(1,263)) \n",
    "        & (F.col('DOLocationID').between(1,263)) & (F.col('passenger_count')\n",
    "        .between(1,4)) & (((F.col('tpep_dropoff_datetime').cast(\"long\")) - \n",
    "        (F.col('tpep_pickup_datetime').cast(\"long\"))) > 0)),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for trips paid by only credit cards \n",
    "\n",
    "sdf_yellow_train.createOrReplaceTempView('yellow_train')\n",
    "\n",
    "sdf_yellow_train = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM \n",
    "    yellow_train\n",
    "WHERE\n",
    "    Payment_type = 1 AND is_valid_record IS TRUE\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the values of the rate code ids \n",
    "\n",
    "sdf_yellow_train = sdf_yellow_train.withColumn('rate_code',\n",
    " \n",
    "    F.when((F.col('RateCodeID') == 1 ), 'Standard')\\\n",
    "    .when((F.col('RateCodeID') == 2 ), 'JFK')\\\n",
    "    .when((F.col('RateCodeID') == 3 ), 'Newark')\\\n",
    "    .when((F.col('RateCodeID') == 4 ), 'Nasau or Westchester')\\\n",
    "    .when((F.col('RateCodeID') == 5 ), 'Negotiated fare')\\\n",
    "    .when((F.col('RateCodeID') == 6 ), 'Shared ride')\\\n",
    "    .otherwise('Standard')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the taxi type column \n",
    "\n",
    "sdf_yellow_train.createOrReplaceTempView('temp_yellow')\n",
    "\n",
    "sdf_yellow_train = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    *,\n",
    "    'Yellow taxi' AS vehicle_type\n",
    "FROM \n",
    "    temp_yellow\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column to identify the ride type in the yellow taxi dataset\n",
    "\n",
    "sdf_yellow_train = sdf_yellow_train.withColumn('vehicle_and_ride_type', \n",
    "    F.when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Standard')), 'Yellow-Standard') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Shared ride')), 'Yellow-Shared ride') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'JFK')), 'Yellow-JFK') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Negotiated fare')), 'Yellow-Negotiated fare') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Newark')), 'Yellow-Newark') \\\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Nasau or Westchester')), \n",
    "    'Yellow-Nasau or Westchester') \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HVFHV Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the HVFHV raw test data directory\n",
    "\n",
    "main_dir = '../data/raw/raw_train/FHVHV/'\n",
    "mth = range(1,13)\n",
    "yr = '2021'\n",
    "\n",
    "# Define the schema \n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "sch = spark.read.parquet('../data/raw/raw_train/FHVHV/2021-01.parquet')\n",
    "sdf_FHVHV_train = spark.createDataFrame(emptyRDD, sch.schema )\n",
    "\n",
    "# Converting the data from 2021 into a single spark dataframe\n",
    "for month in mth:\n",
    "    if month < 10:\n",
    "         month = str(month).zfill(2)\n",
    "    sdf = spark.read.parquet(f'{main_dir}{yr}-{month}.parquet')\n",
    "\n",
    "    #the airport_fee column has different data types in different files\n",
    "    #Hence converting into a same data type and joining the dataframes into a \n",
    "    # single dataframe\n",
    "\n",
    "    sdf_updated_FHVHV = sdf.withColumn(\n",
    "        'airport_fee',\n",
    "        F.col('airport_fee').cast('DOUBLE')\n",
    "    )\n",
    "\n",
    "    sdf_FHVHV_train = sdf_FHVHV_train.unionByName(sdf_updated_FHVHV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the null and nan values\n",
    "\n",
    "sdf_FHVHV_train.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "for c in (sdf_FHVHV_train.columns)[3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling all the numeric columns with 0 inplace of the NULLs\n",
    "\n",
    "sdf_FHVHV_train = sdf_FHVHV_train.fillna(value = 0.0, \n",
    "subset=['base_passenger_fare','base_passenger_fare', 'tolls', 'bcf', \n",
    "'sales_tax', 'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only Uber trips\n",
    "\n",
    "sdf_FHVHV_train.createOrReplaceTempView('FHVHV_view')\n",
    "\n",
    "sdf_FHVHV_train = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM\n",
    "    FHVHV_view\n",
    "WHERE \n",
    "    hvfhs_license_num = 'HV0003'\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Checking for the null values not \n",
    "\n",
    "cols = ['hvfhs_license_num', 'PULocationID', 'DOLocationID']\n",
    "sdf_FHVHV_train.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    " for c in cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the pickup and dropoff location ids to integer\n",
    "\n",
    "for field in ('PU', 'DO'):\n",
    "\n",
    "    field = f'{field}LocationID'\n",
    "\n",
    "    sdf_FHVHV_train = sdf_FHVHV_train.withColumn(\n",
    "        field,\n",
    "\n",
    "        F.col(field).cast('INT')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the numeric columns to double\n",
    "\n",
    "cols = ['base_passenger_fare', 'tolls', 'bcf', 'sales_tax', \n",
    "'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay']\n",
    "\n",
    "for column in cols:\n",
    "    \n",
    "    sdf_FHVHV_train = sdf_FHVHV_train.withColumn(\n",
    "            column,\n",
    "            F.col(column).cast('DOUBLE')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total fare amount\n",
    "\n",
    "sdf_FHVHV_train = sdf_FHVHV_train.withColumn(\n",
    "    'total_amount',\n",
    "\n",
    "     (F.col('base_passenger_fare') + F.col('tolls') +\n",
    "     F.col('bcf')+ F.col('sales_tax') + F.col('congestion_surcharge') + \n",
    "     F.col('airport_fee') + F.col('tips'))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding the total amount value to 2 digits\n",
    "\n",
    "sdf_FHVHV_train = sdf_FHVHV_train.withColumn(\n",
    "    'total_amount',\n",
    "    \n",
    "    F.round('total_amount', 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for outliers and labelling the records as valid or invalid based\n",
    "# on the underlying rules\n",
    "\n",
    "sdf_FHVHV_train = sdf_FHVHV_train.withColumn(\n",
    "    'is_valid_record',\n",
    "\n",
    "    F.when(\n",
    "         ((F.col('total_amount') > 0) & (F.col('PULocationID').between(1,263)) \n",
    "         & (F.col('DOLocationID').between(1,263)) &\n",
    "         (((F.col('dropoff_datetime').cast(\"long\")) - \n",
    "         (F.col('pickup_datetime').cast(\"long\"))) > 0)),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling the rides as Uber rudes\n",
    "\n",
    "sdf_FHVHV_train = sdf_FHVHV_train.withColumn('vehicle_type', \n",
    "    F.when((F.col('hvfhs_license_num') == 'HV0003'), 'Uber')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling the ride type based on the differnet flags\n",
    "\n",
    "sdf_FHVHV_train = sdf_FHVHV_train.withColumn('rate_code', \n",
    "\n",
    "    F.when(((F.col('airport_fee') == 0.0) & \n",
    "    ((F.col('shared_request_flag') == 'Y') & \n",
    "    (F.col('shared_match_flag') == 'Y'))), 'Shared ride')\\\n",
    "\n",
    "    .when(((F.col('airport_fee') == 0.0) & \n",
    "    ((F.col('shared_request_flag') == 'Y') & \n",
    "    (F.col('shared_match_flag') == 'N'))), 'Shared ride')\\\n",
    "\n",
    "    .when(((F.col('airport_fee') == 0.0) & \n",
    "    ((F.col('shared_request_flag') == 'N') & \n",
    "    (F.col('shared_match_flag') == 'Y'))), 'Standard')\\\n",
    "\n",
    "    .when(((F.col('airport_fee') == 0.0) & \n",
    "    ((F.col('shared_request_flag') == 'N') & \n",
    "    (F.col('shared_match_flag') == 'N'))), 'Standard')\\\n",
    "        \n",
    "    .when((F.col('airport_fee') > 0.0 ),'LaGuardia/Newark/JFK')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unnecessary features\n",
    "\n",
    "sdf_FHVHV_train = sdf_FHVHV_train.drop('Hvfhs_license_num',\n",
    "'Dispatching_base_num'\n",
    ",'originating_base_num','request_datetime', 'on_scene_datetime', \n",
    "'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag',\n",
    "'wav_request_flag','wav_match_flag','driver_pay', 'tolls','bcf','sales_tax',\n",
    "'congestion_surcharge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_FHVHV_train = sdf_FHVHV_train.withColumn('vehicle_and_ride_type', \n",
    "    when(((F.col('vehicle_type') == 'Uber') & \n",
    "    (F.col('rate_code') == 'Standard')), 'Uber-Standard') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Uber') & \n",
    "    (F.col('rate_code') == 'Shared ride')), 'Uber-Shared ride') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Uber') & \n",
    "    (F.col('rate_code') == 'LaGuardia/Newark/JFK')), \n",
    "    'Uber-LaGuardia/Newark/JFK')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordering the columns from the Yellow taxi dataset\n",
    "\n",
    "sdf_yellow_train.createOrReplaceTempView(\"final_yellow\")\n",
    "\n",
    "final_sdf_yellow_train = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    tpep_pickup_datetime AS pickup_time,\n",
    "    tpep_dropoff_datetime AS dropoff_time,\n",
    "    Trip_distance AS trip_distance,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    Fare_amount AS base_fare,\n",
    "    Tip_amount AS tips,\n",
    "    Total_amount AS total_amount,\n",
    "    vehicle_type,\n",
    "    rate_code,\n",
    "    vehicle_ride_type\n",
    "FROM\n",
    "    final_yellow\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordering the columns from the HFVHV taxi dataset\n",
    "\n",
    "sdf_FHVHV_train.createOrReplaceTempView(\"final_FHVHV\")\n",
    "\n",
    "final_sdf_FHVHV_train = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    Pickup_datetime AS pickup_time,\n",
    "    DropOff_datetime AS dropoff_time,\n",
    "    trip_miles AS trip_distance,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    base_passenger_fare AS base_fare,\n",
    "    tips,\n",
    "    total_amount,\n",
    "    vehicle_type,\n",
    "    rate_code,\n",
    "    vehicle_and_ride_type\n",
    "FROM\n",
    "    final_FHVHV\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Yellow taxi and HFVHV trips dataset\n",
    "\n",
    "merged_data = final_sdf_yellow_train.union(final_sdf_FHVHV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting year from the timestamp\n",
    "\n",
    "merged_data = merged_data.withColumn(\"Year\", \n",
    "date_format('pickup_time', 'yyyy'))\n",
    "\n",
    "# Extracting month from the timestamp\n",
    "\n",
    "merged_data = merged_data.withColumn(\"Month\", \n",
    "date_format('pickup_time', 'MMMM'))\n",
    "\n",
    "# Extracting date from the timestamp\n",
    "\n",
    "merged_data = merged_data.withColumn(\"Date\", \n",
    "date_format('pickup_time', 'dd'))\n",
    "\n",
    "# Extracting day from the timestamp\n",
    "merged_data = merged_data.withColumn(\"Day\", \n",
    "date_format('pickup_time', 'EEEE'))\n",
    "\n",
    "# Extracting pickup hour from the timestamp\n",
    "\n",
    "merged_data = merged_data.withColumn(\"pickup_hour\", \n",
    "date_format('pickup_time', 'HH'))\n",
    "\n",
    "# Dropping the timestamp columns\n",
    "\n",
    "merged_data = merged_data.drop('pickup_time', 'dropoff_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranigng the columns from the merged dataset\n",
    "\n",
    "merged_data.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "merged_data = spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    Year, Month, Date, Day, pickup_hour,\n",
    "    trip_distance, PULocationID, DOLocationID, base_fare, tips, total_amount, \n",
    "    vehicle_type, rate_code, vehicle_and_ride_type\n",
    "FROM \n",
    "    temp\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the standard rides for both the taxi types\n",
    "\n",
    "final_merged = merged_data.where(\n",
    "\n",
    "    (F.col('vehicle_and_ride_type') == 'Yellow-Standard')\n",
    "    |\n",
    "    (F.col('vehicle_and_ride_type') == 'Uber-Standard')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unwanted columns \n",
    "\n",
    "final_merged = final_merged.drop('vehicle_type', 'rate_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the merged training file\n",
    "\n",
    "final_merged.write.parquet(\"../data/curated/merged_training.paraquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afcda84b7471a9b9fde108a34f159f021985318d3feb99cad4970c959fa9ac9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
