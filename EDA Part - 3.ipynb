{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kasturi/opt/anaconda3/lib/python3.9/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/21 18:02:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/08/21 18:02:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/08/21 18:02:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/08/21 18:02:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZon\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test directory\n",
    "\n",
    "main_dir = '../data/raw/raw_test/yellow/'\n",
    "mth = range(1,5)\n",
    "yr = '2022'\n",
    "\n",
    "# Define the schema for the spark dataframe\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "sch = spark.read.parquet('../data/raw/raw_test/yellow/2022-01.parquet')\n",
    "sdf_yellow_test = spark.createDataFrame(emptyRDD, sch.schema )\n",
    "\n",
    "# Merging the test data from 2022 into one single dataframe\n",
    "\n",
    "for month in mth:\n",
    "    \n",
    "    month = str(month).zfill(2)\n",
    "    sdf = spark.read.parquet(f'{main_dir}{yr}-{month}.parquet')\n",
    "\n",
    "    # The airport_fee column has different data types in different files\n",
    "    # Hence converting into a same data type and joining the dataframes into a \n",
    "    # single dataframe\n",
    "\n",
    "    sdf_updated = sdf.withColumn(\n",
    "        'airport_fee',\n",
    "        F.col('airport_fee').cast('DOUBLE')\n",
    "    )\n",
    "\n",
    "    sdf_yellow_test = sdf_yellow_test.unionByName(sdf_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the null and nan values\n",
    "\n",
    "sdf_yellow_test.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "for c in (sdf_yellow_test.columns)[3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the null values in the airport_fee and congestion surcharge column \n",
    "# with 0s\n",
    "sdf_yellow_test = sdf_yellow_test.fillna(value = 0.0, subset=['airport_fee', \n",
    "'congestion_surcharge'])\n",
    "\n",
    "#Dropping the null values from dataframe\n",
    "\n",
    "sdf_yellow_test = sdf_yellow_test.dropna()\n",
    "sdf_yellow_test.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "for c in (sdf_yellow_test.columns)[3:]]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the pickup and dropoff location ids to integer\n",
    "\n",
    "for field in ('PU', 'DO'):\n",
    "    field = f'{field}LocationID'\n",
    "    sdf_yellow_test = sdf_yellow_test.withColumn(\n",
    "        field,\n",
    "        F.col(field).cast('INT')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the invalid trips\n",
    "\n",
    "sdf_yellow_test = sdf_yellow_test.withColumn(\n",
    "    'is_valid_record',\n",
    "   \n",
    "    F.when(\n",
    "        ((F.col('total_amount') > 0) & (F.col('PULocationID').between(1,263)) \n",
    "        & (F.col('DOLocationID').between(1,263)) & \n",
    "        (F.col('passenger_count').between(1,4)) & \n",
    "        (((F.col('tpep_dropoff_datetime').cast(\"long\")) - \n",
    "        (F.col('tpep_pickup_datetime').cast(\"long\"))) > 0)),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for trips paid by only credit cards \n",
    "\n",
    "sdf_yellow_test.createOrReplaceTempView('yellow_test')\n",
    "\n",
    "sdf_yellow_test = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM \n",
    "    yellow_test\n",
    "WHERE\n",
    "    Payment_type = 1 AND is_valid_record IS TRUE\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rate codes\n",
    "\n",
    "sdf_yellow_test = sdf_yellow_test.withColumn('rate_code', \n",
    "    F.when((F.col('RateCodeID') == 1 ), 'Standard')\\\n",
    "    .when((F.col('RateCodeID') == 2 ), 'JFK')\\\n",
    "    .when((F.col('RateCodeID') == 3 ), 'Newark')\\\n",
    "    .when((F.col('RateCodeID') == 4 ), 'Nasau or Westchester')\\\n",
    "    .when((F.col('RateCodeID') == 5 ), 'Negotiated fare')\\\n",
    "    .when((F.col('RateCodeID') == 6 ), 'Shared ride')\\\n",
    "    .otherwise('Standard')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the taxi type column \n",
    "\n",
    "sdf_yellow_test.createOrReplaceTempView('temp_yellow')\n",
    "\n",
    "sdf_yellow_test = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    *,\n",
    "    'Yellow taxi' AS vehicle_type\n",
    "FROM \n",
    "    temp_yellow\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ride type\n",
    "\n",
    "sdf_yellow_test = sdf_yellow_test.withColumn('vehicle_and_ride_type', \n",
    "    F.when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Standard')), 'Yellow-Standard') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Shared ride')), 'Yellow-Shared ride') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'JFK')), 'Yellow-JFK') \\\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Negotiated fare')), 'Yellow-Negotiated fare') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Newark')), 'Yellow-Newark') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Yellow taxi') & \n",
    "    (F.col('rate_code') == 'Nasau or Westchester')), 'Yellow-Nasau or Westchester')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FHVHV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HVFVHV test data directory\n",
    "\n",
    "main_dir = '../data/raw/raw_test/FHVHV/'\n",
    "mth = range(1,5)\n",
    "yr = '2022'\n",
    "\n",
    "# Define the schema for the spark dataframe\n",
    "\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "sch = spark.read.parquet('../data/raw/raw_test/FHVHV/2022-01.parquet')\n",
    "sdf_FHVHV_test = spark.createDataFrame(emptyRDD, sch.schema )\n",
    "\n",
    "# Make a single dataframe for the FHVHV testing data\n",
    "for month in mth:\n",
    "    \n",
    "    month = str(month).zfill(2)\n",
    "    sdf = spark.read.parquet(f'{main_dir}{yr}-{month}.parquet')\n",
    "\n",
    "    #the airport_fee column has different data types in different files\n",
    "    #Hence converting into a same data type and joining the dataframes into a \n",
    "    # single dataframe  \n",
    "    sdf_updated_FHVHV = sdf.withColumn(\n",
    "        'airport_fee',\n",
    "        F.col('airport_fee').cast('DOUBLE')\n",
    "    )\n",
    "\n",
    "    sdf_FHVHV_test = sdf_FHVHV_test.unionByName(sdf_updated_FHVHV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling all the numeric columns with 0 inplace of the NULLs\n",
    "\n",
    "sdf_FHVHV_test= sdf_FHVHV_test.fillna(value = 0.0, \n",
    "subset=['base_passenger_fare','base_passenger_fare', 'tolls', 'bcf', \n",
    "'sales_tax', 'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only Uber rides\n",
    "\n",
    "sdf_FHVHV_test.createOrReplaceTempView('FHVHV_view')\n",
    "\n",
    "sdf_FHVHV_test = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM\n",
    "    FHVHV_view\n",
    "WHERE \n",
    "    hvfhs_license_num = 'HV0003' \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the null and nan values\n",
    "\n",
    "cols = ['hvfhs_license_num', 'PULocationID', 'DOLocationID']\n",
    "\n",
    "sdf_FHVHV_test.select([count(when(isnan(c) | col(c).isNull(), \n",
    "c)).alias(c) for c in cols]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the pickup and dropoff location ids to integer\n",
    "\n",
    "for field in ('PU', 'DO'):\n",
    "    field = f'{field}LocationID'\n",
    "    sdf_FHVHV_test = sdf_FHVHV_test.withColumn(\n",
    "        field,\n",
    "        F.col(field).cast('INT')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the numeric columns to double\n",
    "\n",
    "cols = ['base_passenger_fare', 'tolls', 'bcf', 'sales_tax', \n",
    "'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay']\n",
    "\n",
    "for column in cols:\n",
    "    sdf_FHVHV_test = sdf_FHVHV_test.withColumn(\n",
    "            column,\n",
    "            F.col(column).cast('DOUBLE')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculting the total fare amount\n",
    "\n",
    "sdf_FHVHV_test = sdf_FHVHV_test.withColumn(\n",
    "    'total_amount', (F.col('base_passenger_fare') + F.col('tolls') + \n",
    "    F.col('bcf')+ F.col('sales_tax') + F.col('congestion_surcharge') + \n",
    "    F.col('airport_fee') + F.col('tips'))\n",
    ")\n",
    "\n",
    "# Rounding the total amoount to 2 decimal places\n",
    "\n",
    "sdf_FHVHV_test = sdf_FHVHV_test.withColumn(\n",
    "    'total_amount', F.round('total_amount', 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discarding the invalid records\n",
    "\n",
    "sdf_FHVHV_test = sdf_FHVHV_test.withColumn(\n",
    "    'is_valid_record',\n",
    "   \n",
    "    F.when(\n",
    "        ((F.col('total_amount') > 0) & (F.col('PULocationID').between(1,263)) \n",
    "        & (F.col('DOLocationID').between(1,263)) & \n",
    "        (((F.col('dropoff_datetime').cast(\"long\")) - \n",
    "        (F.col('pickup_datetime').cast(\"long\"))) > 0)),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a column with the taxi type\n",
    "\n",
    "sdf_FHVHV_test = sdf_FHVHV_test.withColumn('vehicle_type', \n",
    "\n",
    "    F.when((F.col('hvfhs_license_num') == 'HV0003'), 'Uber')\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling the type of ride\n",
    "\n",
    "sdf_FHVHV_test = sdf_FHVHV_test.withColumn('rate_code', \n",
    "\n",
    "    F.when(((F.col('airport_fee') == 0.0) & \n",
    "    ((F.col('shared_request_flag') == 'Y') & \n",
    "    (F.col('shared_match_flag') == 'Y'))), 'Shared ride')\\\n",
    "\n",
    "    .when(((F.col('airport_fee') == 0.0) & \n",
    "    ((F.col('shared_request_flag') == 'Y') & \n",
    "    (F.col('shared_match_flag') == 'N'))), 'Shared ride')\\\n",
    "\n",
    "    .when(((F.col('airport_fee') == 0.0) & \n",
    "    ((F.col('shared_request_flag') == 'N') & \n",
    "    (F.col('shared_match_flag') == 'Y'))), 'Standard')\\\n",
    "\n",
    "    .when(((F.col('airport_fee') == 0.0) & \n",
    "    ((F.col('shared_request_flag') == 'N') & \n",
    "    (F.col('shared_match_flag') == 'N'))), 'Standard')\\\n",
    "\n",
    "    .when((F.col('airport_fee') > 0.0 ),'LaGuardia/Newark/JFK')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rate code and ride type\n",
    "\n",
    "sdf_FHVHV_test = sdf_FHVHV_test.withColumn('vehicle_and_ride_type', \n",
    "\n",
    "    when(((F.col('vehicle_type') == 'Uber') & \n",
    "    (F.col('rate_code') == 'Standard')), 'Uber-Standard') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Uber') & \n",
    "    (F.col('rate_code') == 'Shared ride')), 'Uber-Shared ride') \\\n",
    "\n",
    "    .when(((F.col('vehicle_type') == 'Uber') & \n",
    "    (F.col('rate_code') == 'LaGuardia/Newark/JFK')), \n",
    "    'Uber-LaGuardia/Newark/JFK')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unwanted columns\n",
    "\n",
    "sdf_FHVHV_test = sdf_FHVHV_test.drop('Hvfhs_license_num','Dispatching_base_num'\n",
    ",'originating_base_num','request_datetime', 'on_scene_datetime', \n",
    "'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag',\n",
    "'wav_request_flag','wav_match_flag','driver_pay', 'tolls','bcf','sales_tax',\n",
    "'congestion_surcharge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_yellow_test.createOrReplaceTempView(\"final_yellow\")\n",
    "\n",
    "final_sdf_yellow_test = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    tpep_pickup_datetime AS pickup_time,\n",
    "    tpep_dropoff_datetime AS dropoff_time,\n",
    "    Trip_distance AS trip_distance,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    Fare_amount AS base_fare,\n",
    "    Tip_amount AS tips,\n",
    "    Total_amount AS total_amount,\n",
    "    vehicle_type,\n",
    "    rate_code,\n",
    "    vehicle_and_ride_type\n",
    "FROM\n",
    "    final_yellow\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_FHVHV_test.createOrReplaceTempView(\"final_FHVHV\")\n",
    "\n",
    "final_sdf_FHVHV_test = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    Pickup_datetime AS pickup_time,\n",
    "    DropOff_datetime AS dropoff_time,\n",
    "    trip_miles AS trip_distance,\n",
    "    PULocationID,\n",
    "    DOLocationID,\n",
    "    base_passenger_fare AS base_fare,\n",
    "    tips,\n",
    "    total_amount,\n",
    "    vehicle_type,\n",
    "    rate_code,\n",
    "    vehicle_and_ride_type\n",
    "FROM\n",
    "    final_FHVHV\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets\n",
    "\n",
    "merged_data = final_sdf_yellow_test.union(final_sdf_FHVHV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month, date, day, pickup hour from the timestamps\n",
    "\n",
    "merged_data = merged_data.withColumn(\"Year\", \n",
    "date_format('pickup_time', 'yyyy'))\n",
    "\n",
    "merged_data = merged_data.withColumn(\"Month\", \n",
    "date_format('pickup_time', 'MMMM'))\n",
    "\n",
    "merged_data = merged_data.withColumn(\"Date\", \n",
    "date_format('pickup_time', 'dd'))\n",
    "\n",
    "merged_data = merged_data.withColumn(\"Day\", \n",
    "date_format('pickup_time', 'EEEE'))\n",
    "\n",
    "merged_data = merged_data.withColumn(\"pickup_hour\", \n",
    "date_format('pickup_time', 'HH'))\n",
    "\n",
    "# Drop the timestamps\n",
    "merged_data = merged_data.drop('pickup_time', 'dropoff_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns of the merged dataset\n",
    "\n",
    "merged_data.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "merged_data = spark.sql(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    Year, Month, Date, Day, pickup_hour,\n",
    "    trip_distance, PULocationID, DOLocationID, base_fare, tips, total_amount, \n",
    "    vehicle_type, rate_code, vehicle_and_ride_type\n",
    "\n",
    "FROM \n",
    "    temp\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the standard rides\n",
    "\n",
    "final_merged = merged_data.where(\n",
    "\n",
    "    (F.col('vehicle_and_ride_type') == 'Yellow-Standard')\n",
    "    |\n",
    "    (F.col('vehicle_and_ride_type') == 'Uber-Standard')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unwanted columns\n",
    "\n",
    "final_merged = final_merged.drop('vehicle_type', 'rate_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the outlier data (downloaded data should be first 4 months only)\n",
    "\n",
    "final_merged.createOrReplaceTempView('outlier')\n",
    "\n",
    "final_merged = spark.sql(\"\"\" \n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM \n",
    "    outlier\n",
    "WHERE \n",
    "    Month IN ('January', 'February', 'March', 'April')\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the merged data\n",
    "\n",
    "final_merged.write.parquet(\"../data/curated/merged_testing.paraquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afcda84b7471a9b9fde108a34f159f021985318d3feb99cad4970c959fa9ac9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
